<!-- 
Author : ChungYi Fu (Kaohsiung, Taiwan)   2020/1/28 15:00
https://www.facebook.com/francefu

Try it!
https://fustyles.github.io/webduino/TensorFlow/Face-api/Face-api_FaceDetection_image.html

How to enable WebGL in Chrome.
https://superuser.com/questions/836832/how-can-i-enable-webgl-in-my-browser
-->
<!DOCTYPE html>
<head>
  <title>Face Detection</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
  <script src='face-api.min.js'></script>
</head>
<body>
<input type="file" id="selectimage" disabled="true"></input>
<br><br>
<img id="OriginImage" style="display:none">
<img id="ShowImage">
<canvas id="canvas" width="0" height="0"></canvas>
<div id="webcam-container"></div>
<br>
<div id="message" style="width:350px;color:red">Please wait for loading model.</div>

<script>
var OriginImage = document.getElementById('OriginImage');
var ShowImage = document.getElementById('ShowImage');
var ShowImageWidth = 500;
var Model;

const modelPath = './';
let currentStream;
let displaySize = { width:320, height: 240 }
let canvas;
let faceDetection;
let message = document.getElementById('message'); 
  
  function ObjectDetect() {
    $(".progress-bar").removeClass('d-none');
    Promise.all([
      faceapi.nets.tinyFaceDetector.load(modelPath),
      faceapi.nets.faceLandmark68TinyNet.load(modelPath),
      faceapi.nets.faceRecognitionNet.load(modelPath),
      faceapi.nets.faceExpressionNet.load(modelPath),
      faceapi.nets.ageGenderNet.load(modelPath)
    ]).then(function(){
      result.innerHTML = "Please select one image.";
      document.getElementById('selectimage').disabled = false;
    })
  }  
                        
  async function DetectImage() {
      const detections = await faceapi.detectAllFaces(ShowImage, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks(true).withFaceExpressions().withAgeAndGender()
      const resizedDetections = faceapi.resizeResults(detections, displaySize)
      canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
      faceapi.draw.drawDetections(canvas, resizedDetections)
      faceapi.draw.drawFaceLandmarks(canvas, resizedDetections)
      faceapi.draw.drawFaceExpressions(canvas, resizedDetections)
      resizedDetections.forEach(result => {
        const { age, gender, genderProbability } = result
        message.innerHTML = JSON.stringify(result);
        new faceapi.draw.DrawTextField(
          [
            `${faceapi.round(age, 0)} years`,
            `${gender} (${faceapi.round(genderProbability)})`
          ],
          result.detection.box.bottomRight
        ).draw(canvas)
      })
      try { 
        document.createEvent("TouchEvent");
        setTimeout(function(){DetectVideo();},250);
      }
      catch(e) { 
        setTimeout(function(){DetectVideo();},150);
      } 
  }

document.getElementById('selectimage').onchange = function (event) {
  var target = event.target || window.event.srcElement;
  var files = target.files;
  if (FileReader && files && files.length) {
    var fr = new FileReader();
    fr.onload = function () {
      result.innerHTML = "Loading image to detect...";  
      OriginImage.src = fr.result;
    }
    fr.readAsDataURL(files[0]);
  }
}
    
document.getElementById('OriginImage').onload = function (event) {
  try { 
    document.createEvent("TouchEvent"); 
    var width = document.body.clientWidth;
  }
  catch(e) { 
    var width = ShowImageWidth;
  } 
  
  if (OriginImage.height<OriginImage.width) {
    var height = width*OriginImage.height/OriginImage.width; 
  }
  else {
    var height = width;
    width = height*OriginImage.width/OriginImage.height; 
  }
  
  var height = width*OriginImage.height/OriginImage.width;
  ShowImage.width = width;
  ShowImage.height = height;  
  ShowImage.src = OriginImage.src;
}
  
document.getElementById('ShowImage').onload = function (event) {
  if (Model) DetectImage(Model);
}
  
window.onload = function () { ObjectDetect(); }
</script>

</body>
</html>
