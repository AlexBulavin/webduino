<!-- 
Author : ChungYi Fu (Kaohsiung, Taiwan)   2020/4/26 20:00
https://www.facebook.com/francefu

Try it!
https://fustyles.github.io/webduino/TensorFlow/FacemeshDetection_video/facemesh_video.html
-->
<!DOCTYPE html>
<head>
  <title>Facemesh Detection (Camera)</title>
  <meta charset="utf-8">
  <meta name="robots" content="noindex">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.7.4/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script>
</head>
<body>
<video id="video" width="320" height="240" preload autoplay loop muted></video>
<canvas id="canvas"></canvas>
<br>
<div id="result" style="color:red">Please wait for loading model.</div>
  
<script> 
  var video = document.getElementById('video');
  var canvas = document.getElementById('canvas'); 
  var context = canvas.getContext('2d');
  var result = document.getElementById('result');
  let Model; 
  
  HandDetect();
  
  function HandDetect() {
    facemesh.load().then(function(net) {
		Model = net
		console.log(Model);
		result.innerHTML = "";
		startvideo();
    });
  }
  
  function startvideo() {
    video.style.visibility="hidden";
    video.style.position="absolute";
    navigator.mediaDevices
      .getUserMedia({
        audio: false,
        video: {
          facingMode: "user",
          width: 320,
          height: 240
        }
      })
      .then(stream => {
        video.srcObject = stream
        video.onloadedmetadata = () => {       
          video.play();
          canvas.setAttribute("width", video.width);
          canvas.setAttribute("height", video.height);          
          setTimeout(function(){DetectVideo(); }, 100);
        }
      })   
  }
  
  async function DetectVideo() {
    context.translate((canvas.width + video.width) / 2, 0);
    context.scale(-1, 1);
    context.drawImage(video, 0, 0, video.width, video.height);
    context.setTransform(1, 0, 0, 1, 0, 0);
    
    await Model.estimateFaces(canvas).then(predictions => {
      result.innerHTML = "";
		
		if (predictions.length > 0) {
			var part = ["leftCheek","leftEyeLower0","leftEyeLower1","leftEyeLower2","leftEyeLower3","leftEyeUpper0","leftEyeUpper1","leftEyeUpper2","leftEyebrowLower","leftEyebrowUpper","lipsLowerInner","lipsLowerOuter","lipsUpperInner","lipsUpperOuter","midwayBetweenEyes","noseBottom","noseLeftCorner","noseRightCorner","noseTip","rightCheek","rightEyeLower0","rightEyeLower1","rightEyeLower2","rightEyeLower3","rightEyeUpper0","rightEyeUpper1","rightEyeUpper2","rightEyebrowLower","rightEyebrowUpper","silhouette"];
			for (let k = 0; k < predictions.length; k++) {
			  const annotations = predictions[k].annotations;
			  const boundingBox = predictions[k].boundingBox;
			  for (let j = 0; j < part.length; j++) {
				  for (let i = 0; i < annotations[part[j]].length; i++) {
					result.innerHTML += k+","+part[j]+","+i+","+annotations[part[j]][i][0]+","+annotations[part[j]][i][1]+","+annotations[part[j]][i][2]+","+boundingBox.topLeft[0][0]+","+boundingBox.topLeft[0][1]+","+boundingBox.bottomRight[0][0]+","+boundingBox.bottomRight[0][1]+"<br>";
				  }
			  }
			  const res = predictions[k].scaledMesh;
			  drawKeypoints(context, res, predictions[k].annotations);
			}
		}

      setTimeout(function(){DetectVideo(); }, 100);
    });
  }

function drawPoint(ctx, y, x, r) {
  ctx.beginPath();
  ctx.fillStyle = "red";
  ctx.arc(x, y, r, 0, 2 * Math.PI);
  ctx.fill();
}

function drawKeypoints(ctx, keypoints) {
  const keypointsArray = keypoints;
  for (let i = 0; i < keypointsArray.length; i++) {
    const y = keypointsArray[i][0];
    const x = keypointsArray[i][1];
    drawPoint(ctx, x, y, 1);
  }
}

</script>

</body>
</html>
